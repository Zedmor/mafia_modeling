I am trying to build model of the game and find optimal strategy for each player and position.

Mafia Game description:

Game is played with 10 players, players are getting roles at random.

At the beginning of the game there's 3 players who gets Black cards (1 Don and 2 Mafia) and 7 players get Red cards
(6 Citizen card and 1 Sheriff card).

One team is playing against each other.
Three black players knows each other and red players do not know who is red and who is black.

Game is played with phases - "Day" and "Night". During day phase players are stating their position
 and make declarations, as well as put other player to the vote. Game starts with the day and player 1 making his statement.

Type of declarations players make (for this model):

1. "I think player X is a red player"
2. "I think player Y is a black player"
3. I am sheriff and checked player Z and he is black
4. I am sheriff and checked player M and he is red.
5. I am putting player K on the vote

Player can make as many declarations as he like (up to 9 declarations on the possible opinion and sheriff checks has to be believable
(if there's only two nights and player states has 3 checks that is suspicious)

This information is available to all the players.

After day round of more than 1 players are been put to the vote, players vote and if one player gets majority he is eliminated from the game.
Eliminated player has opportunity to state his position.

During night Don decide who to kill in the game and that player is getting eliminated. If Don is not in the game first Mafia decides who to kill,
if only one mafia left, that player decides who to kill.

Then Don of the game if he is in the game can check if certain player is a sheriff card, he is getting back boolean True/False response.
Then Sheriff of the game, if he is in the game, can check if certain player is a mafia and getting response True/False (is that player mafia or not).

This information is private to the player.

After night killed player gets opportunity to make statement. After that next player starts round
(for example in first round player 1 starts the round, if player 2 killed on the first night, player 3 starts the round).

Game finishes with two possible outcomes:
1.  If there's even number of red and black players remains after vote or night Mafia (Black team) wins
3.  If all mafia is eliminated red team wins


I want to model this game with reinforcmenet learning and I understand that I will probably need 4 different agents (Citizen, Sheriff, Mafia and Don).

I understand what value function could be, since Mafia can only win when it will vote out 2 "Red" players and Red players will win if they will vote out all three Mafia players, that means we can offer awards every time when member of "enemy" team leaves during vote.

What I do not undertand is how to model those "I think x is a red/black player" messages. How I can create policy that will take those into account?


---
Small model game:

>>>Is it a good idea to start with a smaller model of the game? For example
    we have one citizen, one mafia and one sheriff. Sheriff knows who is ma
   fia. Citizen player do not know who is mafia and who is a sheriff. Then
   players are making declarations (one round) and then make a vote (if 2 p
   layers vote against one and that player is a mafia citizen side wins oth
   erwise mafia wins). Is it reasonable to start with this super simplified
    game?

Absolutely, starting with a simplified version of the game is a great idea. It allows you to focus on the core mechanics and algorithms without getting overwhelmed by the complexity of the full game. Once you have a working model for the simplified game, you can gradually add more complexity.

In the simplified game you described, the strategy would involve deciding what declarations to make and who to vote against. The game state would include the roles of the players and the declarations made so far.

Here's how you might define the game state and the strategy:

```python
class GameState:
    def __init__(self, roles, declarations):
        self.roles = roles
        self.declarations = declarations

class Strategy:
    def __init__(self):
        self.declaration_strategy = {}
        self.voting_strategy = {}

    def decide_declaration(self, game_state):
        # Decide what declaration to make based on the game state
        pass

    def decide_vote(self, game_state):
        # Decide who to vote against based on the game state
        pass
```

In this structure, the `GameState` class represents the state of the game, which includes the roles of the players and the declarations made so far. The `Strategy` class represents the strategy of a player, which includes a declaration strategy and a voting strategy. The `decide_declaration` and `decide_vote` methods decide what declaration to make and who to vote against, respectively, based on the game state.

You can then use the CFR algorithm to find the optimal strategy in this simplified game. Once you have a working model for the simplified game, you can gradually add more complexity, such as more players, more roles, and more rounds.


Sure, here's a high-level plan to model this simplified game:

1. **Define the Game State**: The game state should include all the information that is needed to make decisions. This includes the roles of all players, the declarations made by each player, and the players who have been put to vote.

2. **Define the Actions**: The actions that a player can take depend on their role. For example, a player can make declarations and put other players to the vote.

3. **Define the Transition Function**: The transition function takes the current game state and an action, and returns the new game state. For example, if the action is to make a declaration, the new game state should reflect that this declaration has been made.

4. **Define the Reward Function**: The reward function takes the current game state and an action, and returns a reward. The reward should be positive if the action leads to a win for the player's team, and negative otherwise.

5. **Implement the CFR Algorithm**: The Counterfactual Regret Minimization (CFR) algorithm is a self-play algorithm that can be used to find the optimal strategy in imperfect-information games. The algorithm iteratively updates the strategy by minimizing the regret of not having played the best response in the past.

6. **Simulate the Game**: Use the CFR algorithm to simulate the game multiple times. After each simulation, update the strategy using the regret information.

7. **Evaluate the Strategy**: After a sufficient number of simulations, the strategy should converge to the game-theoretic optimal strategy. Evaluate the strategy by playing the game with the strategy and observing the win rate.

This is a high-level plan and each step can be further broken down into smaller tasks. Also, note that this is a complex problem and may require a significant amount of computational resources to solve.


------

The state of the game in this context could include the following components:

1. **Player Roles**: The roles of each player that have been revealed so far.

2. **Player Status**: Whether each player is still in the game or has been eliminated.

3. **Game Phase**: Whether it's currently the "Day" or "Night" phase of the game.

4. **Belief Matrix**: A matrix representing the beliefs of each player about the roles of the other players.

5. **Credibility Factors**: A vector representing the credibility of each player's declarations.

6. **Sheriff Belief Matrix**: A matrix representing the beliefs of each player about who the sheriff is (if you decide to include this in your model).

7. **Turn Order**: The order in which players will make their declarations or votes.

8. **Previous Actions**: The actions taken by each player in the previous rounds, such as who they voted for or if they made any declarations.

Remember, the state should capture all the information that is relevant for making decisions in the game. Depending on the complexity of your model and the computational resources available, you might choose to include more or less information in the state.
----- Belief matrix
>>>Lets talk a little about declarations and belief matrix. How I can make that every declaration of the play
   er change our belief not only about other players but about their declarations. For example if someone had
    been called as "Red" player by multiple players it's more probably that his declaration are more "truthfu
   l" than other players. Is it possible to recalculate belief matrix based on every declaration made? I beli
   eve that could be computationally expensive since there's graph of declarations (who says about who and wh
   en). How to deal with it?
You're correct that incorporating the credibility of each player's declarations into the belief matrix can ad
d a significant layer of complexity to the model. One way to handle this is to use a credibility factor for e
ach player, as discussed earlier. This factor could be updated each time a player makes a declaration, and us
ed to weight the impact of that player's declarations on the belief matrix.

However, recalculating the entire belief matrix based on every declaration made can indeed be computationally
 expensive, especially as the number of players and declarations increases. To mitigate this, you could consi
der a few strategies:

1. **Incremental Updates**: Instead of recalculating the entire belief matrix each time a declaration is made
, you could update only the relevant parts of the matrix. For example, if a player makes a declaration about
another player, you could update only the row and column corresponding to those two players.

2. **Batch Updates**: Instead of updating the belief matrix after each individual declaration, you could accu
mulate a batch of declarations and update the matrix all at once. This could reduce the computational cost by
 allowing you to perform the updates in parallel.

3. **Approximation Techniques**: If the exact calculation of the belief matrix is too computationally expensi
ve, you could consider using approximation techniques. For example, you could use a simplified model of the d
eclaration graph that captures the most important features but is less computationally intensive to update.

4. **Limit the Number of Declarations**: To keep the complexity manageable, you could limit the number of dec
larations that each player can make in each round. This would reduce the number of updates needed to the beli
ef matrix.

Remember, the goal is to capture the essential dynamics of the game without making the model so complex that
it becomes intractable. It's okay to make simplifications and approximations as long as they don't significan
tly compromise the accuracy of the model.
------
------ Making declarations
>>>What in our model make players to make declarations? What is incentive for them?
In the context of the Mafia game, making declarations is a crucial part of the gameplay as it allows players
to share their beliefs and suspicions about the roles of other players, influence the voting process, and att
empt to sway the game in favor of their team.

In terms of the reinforcement learning model, the incentive for agents to make declarations can be incorporat
ed into the reward function. For example, you could design the reward function such that agents receive a pos
itive reward when their declarations lead to desirable outcomes (e.g., a Mafia player being voted out as a re
sult of their declaration), and a negative reward when their declarations lead to undesirable outcomes (e.g.,
 a Citizen player being voted out as a result of their declaration).

Furthermore, the policy learned by the reinforcement learning agent would also encourage making declarations.
 If the agent learns that making certain declarations in certain states leads to higher future rewards, it wi
ll be more likely to make those declarations in similar states in the future.

It's also worth noting that making declarations can be a strategic move. For example, a Mafia player might ma
ke a false declaration to cast suspicion on a Citizen player, or a Sheriff player might make a declaration to
 reveal the results of their checks. These strategic considerations can be learned by the reinforcement learn
ing agent through the reward function and the learning process.
In the context of the Mafia game, making declarations is a crucial part of the gameplay as it allows players
to share their beliefs and suspicions about the roles of other players, influence the voting process, and att
empt to sway the game in favor of their team.

In terms of the reinforcement learning model, the incentive for agents to make declarations can be incorporat
ed into the reward function. For example, you could design the reward function such that agents receive a pos
itive reward when their declarations lead to desirable outcomes (e.g., a Mafia player being voted out as a re
sult of their declaration), and a negative reward when their declarations lead to undesirable outcomes (e.g.,
 a Citizen player being voted out as a result of their declaration).

Furthermore, the policy learned by the reinforcement learning agent would also encourage making declarations.
 If the agent learns that making certain declarations in certain states leads to higher future rewards, it wi
ll be more likely to make those declarations in similar states in the future.

It's also worth noting that making declarations can be a strategic move. For example, a Mafia player might ma
ke a false declaration to cast suspicion on a Citizen player, or a Sheriff player might make a declaration to
 reveal the results of their checks. These strategic considerations can be learned by the reinforcement learn
ing agent through the reward function and the learning process.

---------
>>>How are messages "I am sheriff, I checked X and he is black" could be integrated into game state?
These types of messages can be integrated into the game state by updating the belief matrix and the credibili
ty factors.

When a player declares "I am sheriff, I checked X and he is black", you can update the belief matrix to refle
ct this new information. Specifically, you can increase the probability that player X is a black player in th
e rows corresponding to all other players, since this declaration is public and all players are aware of it.

In addition, you can update the credibility factors to reflect the fact that this player is claiming to be th
e sheriff. If a player claims to be the sheriff and their checks turn out to be accurate (i.e., the players t
hey claim are black are voted out and revealed to be black), you can increase their credibility factor. Conve
rsely, if their checks turn out to be inaccurate, you can decrease their credibility factor.

These updates to the belief matrix and the credibility factors can be part of the transition function in the
reinforcement learning model, which determines how the state of the game changes in response to the actions t
aken by the players.

It's also worth noting that these types of messages introduce an additional layer of complexity to the game,
as players can lie about their role or the results of their checks. This can be handled in the reinforcement
learning model by allowing for the possibility of false declarations and incorporating this into the belief m
atrix and the credibility factors.
----------
Game state:

>>>How we can deal with that fact that some information only available for certain roles (agents)? For exampl
   e that checks of Don only available to agent of that role and no one else?
In reinforcement learning, this is typically handled by having different state representations for different
agents, depending on what information they have access to. This is known as partial observability, where each
 agent only has access to a subset of the total state of the environment.

In the context of the Mafia game, each agent would have its own state representation that includes only the i
nformation it has access to. For example, the state representation for the Don agent would include the result
s of the Don's checks, while the state representation for the other agents would not include this information
.

During the learning process, each agent would then update its own state representation based on the actions i
t takes and the observations it makes. For example, if the Don agent checks a player and finds out they are t
he Sheriff, it would update its state representation to include this information.

This approach allows each agent to make decisions based on the information it has access to, while still allo
wing for the possibility of learning from the actions of the other agents. For example, if the Don agent obse
rves that a player is acting suspiciously, it might decide to check that player in the next round, even if it
 doesn't have any direct information about that player's role.
In reinforcement learning, this is typically handled by having different state representations for different
agents, depending on what information they have access to. This is known as partial observability, where each
 agent only has access to a subset of the total state of the environment.

In the context of the Mafia game, each agent would have its own state representation that includes only the i
nformation it has access to. For example, the state representation for the Don agent would include the result
s of the Don's checks, while the state representation for the other agents would not include this information
.

During the learning process, each agent would then update its own state representation based on the actions i
t takes and the observations it makes. For example, if the Don agent checks a player and finds out they are t
he Sheriff, it would update its state representation to include this information.

This approach allows each agent to make decisions based on the information it has access to, while still allo
wing for the possibility of learning from the actions of the other agents. For example, if the Don agent obse
rves that a player is acting suspiciously, it might decide to check that player in the next round, even if it
 doesn't have any direct information about that player's role.

>>>So I will be able to hold "central" game state and then just limit to what kind of actions/parts of that s
   tate each agent have access to?
Yes, exactly. You can maintain a "central" or "global" game state that includes all the information about the
 game, and then each agent would have access to a subset of this state based on their role and the informatio
n they have gathered.

For example, the global state could include the roles of all players, the results of all checks, the status o
f all players (in the game or eliminated), the belief matrix, credibility factors, etc. Then, each agent woul
d have a "local" state that includes only the information they have access to.

For a Citizen or Mafia agent, this might include the status of all players, their own role, the belief matrix
, and the credibility factors. For a Sheriff or Don agent, this might also include the results of their check
s.

When an agent takes an action, it would update both the global state and its local state. For example, if the
 Sheriff agent checks a player and finds out they are a Mafia, it would update the global state to reflect th
is, and also update its local state to include this new information.

This approach allows each agent to make decisions based on the information they have, while still allowing fo
r the possibility of learning from the actions of other agents and the evolution of the global state.

---------- Simplified

I am trying to build model of the game and find optimal strategy for each player and position.

Mafia Game description:

Game is played with 10 players, players are getting roles at random.

At the beginning of the game there's 3 players who gets Black cards (Mafia) and 7 players get Red cards
(Citizens).

One team is playing against each other.
Three black players knows each other and red players do not know who is red and who is black.

Game is played with phases - "Day" and "Night". During day phase players are stating their position
 and make declarations, as well as put other player to the vote. Game starts with the day and player 1 making his statement.

Type of declarations players make (for this model):

1. "I think player X is a red player"
2. "I think player Y is a black player"
3. I am putting player K on the vote

Player can make as many declarations as he like (up to 9 declarations about other players and putting someone to the vote)
This information is available to all the players.

After day round of more than 1 players are been put to the vote,
players cast vote and if one player gets majority he is eliminated from the game.
Eliminated player has opportunity to state his position (up to 9 declarations)

During night first mafia decide who to kill in the game and that player is getting eliminated.

After night killed player gets opportunity to make statement. (up to 9 declarations)
 After that next player starts round
(for example in first round player 1 starts the round, if player 2 killed on the first night, player 3 starts the round).

Game finishes with two possible outcomes:
1.  If there's even number of red and black players remains after vote or night Mafia (Black team) wins
3.  If all mafia is eliminated red team wins


first attempt is to supply it vector of player roles and make sure it will make correct decisions knowing all information (about other players).

I am encoding game state like this with action I about to make and status of other players:

    def get_game_state_vector(self, player, max_actions=100):
        state_vector = []

        # state_vector.append(player.role.index())
        for p in self.players:
            state_vector.append(int(p.is_alive))
            state_vector.append(int(p in self.nominated_players))
            state_vector.append(p.team.index())
            if player == p:
                state_vector.append(1)
            else:
                state_vector.append(0)

        return torch.tensor(state_vector, dtype=torch.float32)

    def get_state_vector(self, player, action_type):
        # Convert the action type to a one-hot encoded vector
        action_vector = [0, 0, 0, 0]
        if action_type == "make_declarations":
            action_vector[0] = 1
        elif action_type == "vote":
            action_vector[1] = 1
        elif action_type == "kill":
            action_vector[2] = 1
        elif action_type == "nominate_player":
            action_vector[3] = 1

        state_vector = self.get_game_state_vector(player)

        # Concatenate the action vector and state vector
        full_vector = torch.cat(
            (torch.tensor(action_vector, dtype=torch.float32), state_vector)
        )

        return full_vector


I decode action vector this way masking parts of the vector to select action I am taking:

class NeuralNetworkCitizenPolicy(Policy):
    policy_name = "NeuralNetworkCitizenPolicy"

    def __init__(self, num_players):
        self.num_players = num_players
        try:
            logger.info("Loading model...")
            self.network = torch.load(MODEL_PATH).to(device)
            self.target_net = torch.load(MODEL_PATH).to(device)
        except Exception:
            logger.error("Error in loading model")
            self.network = NeuralNetwork(INPUT_LAYER_SIZE, self.output_size).to(device)
            self.target_net = NeuralNetwork(INPUT_LAYER_SIZE, self.output_size).to(device)

        self.optimizer = optim.Adam(self.network.parameters(), lr=0.00025)
        self.criterion = nn.MSELoss()
        self.epsilon = 1
        self.epsilon_decay = 0.995

    @property
    def output_size(self):
        """
        Output is vector of:
        10 for voting,
        11 for nominating (players + no one),
        63 for declaring (three declarations for each target player with mafia/citizen + option for not declaring).
        10 for killing
        ----
        94 long

        :return:
        """
        return 94
        # return self.num_players + self.num_players + 1 + (self.num_players * 2 + 1) * 3 + self.num_players

    def get_action_vector(self, state_vector):
        if random.random() < self.epsilon:
            # Generate a random action vector
            action_vector = torch.rand(self.output_size)
        else:
            # Use the network to generate the action vector
            with torch.no_grad():
                action_vector = self.network(state_vector.unsqueeze(0))
        return action_vector.squeeze(0)

    def _get_declarations_from_vector(self, action_vector, game_state):
        # The action vector should already be masked, so we just need to find the indices of the maximum values.
        # These indices correspond to the declarations that the agent has decided to make.
        top_indices = torch.topk(action_vector, 3).indices  # Get the indices of the top 3 values
        declarations = []
        for index in top_indices:
            index -= 21
            if index < 60:
                target = game_state.players[index % 10]
                belief = index % 20 // 10
                if belief == 0:
                    belief = Role.MAFIA
                else:
                    belief = Role.CITIZEN

                declarations.append((target, belief))

        return declarations, top_indices

    def _get_vote_from_vector(self, action_vector, game_state):
        if not game_state.nominated_players:
            return None, None

        vote_index = torch.argmax(action_vector).item()
        voted_player = game_state.players[vote_index]

        return voted_player, torch.Tensor([vote_index])

    def _get_kill_from_vector(self, action_vector):
        # TODO: Implement this method to convert the output vector of the network into a kill
        pass

    def _get_nomination_from_vector(self, action_vector, game_state):
        # The action vector should already be masked, so we just need to find the index of the maximum value.
        # This index corresponds to the player that the agent has decided to nominate.
        nomination_index = torch.argmax(action_vector).item()
        # The nomination_index should be between 0 and 10 (inclusive) because there are 10 players and one 'no one' option.
        # We can use this index to get the corresponding player from the game state.
        if nomination_index < 10:
            nominated_player = game_state.players[nomination_index]
        else:
            nominated_player = None  # The agent has decided not to nominate anyone

        return nominated_player, torch.Tensor([nomination_index])

    def get_vector(self, game_state, action_type, player):
        state_vector = game_state.get_state_vector(player, action_type)
        action_vector = self.get_action_vector(state_vector)
        mask = game_state.create_mask(action_type)
        action_vector = action_vector.masked_fill(~mask, -1e9)
        player.action_vector = action_vector
        return action_vector

    def make_declarations(self, game_state, player):
        action_type = "make_declarations"
        action_vector = self.get_vector(game_state, action_type, player)
        declarations, action_result_indices = self._get_declarations_from_vector(action_vector, game_state)
        player.action_result.index_fill_(0, action_result_indices.long(), 1)
        return declarations

    def vote(self, game_state, player):
        action_type = "vote"
        action_vector = self.get_vector(game_state, action_type, player)
        target_player, action_result_indices = self._get_vote_from_vector(action_vector, game_state)
        if not target_player:
            return
        player.action_result.index_fill_(0, action_result_indices.long(), 1)
        return target_player

    def kill(self, game_state, player):
        action_type = "kill"
        action_vector = self.get_vector(game_state, action_type, player)
        target_player, action_result_indices = self._get_kill_from_vector(action_vector)
        player.action_result.index_fill_(0, action_result_indices.long(), 1)
        return target_player

    def nominate_player(self, game_state, player):
        action_type = "nominate_player"
        action_vector = self.get_vector(game_state, action_type, player)
        target_player, action_result_indices = self._get_nomination_from_vector(action_vector, game_state)
        player.action_result.index_fill_(0, action_result_indices.long(), 1)
        return target_player


What would be a suitable neural network architecture?

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.layer1 =

    def forward(self, x):
        out =


----

I want to model this game with reinforcement learning.

I understand what value function could be, since Mafia can only win when it will vote out 2 "Red" players and Red players will win if they will vote out all three Mafia players, that means we can offer awards every time when member of "enemy" team leaves during vote.

What I do not understand is how to model those "I think x is a red/black player" messages. How I can create policy that will take those into account?

I will probably need 2 networks:
First network will make a decision what to say and what not to, who to put to vote. For black player agent that network should make decision who to nominate, call black and red players and vote against based on maximization of probability to win (black player need to make actions to maximize it's positive outcome) and for red players it should dictate who to nominate and vote based on belief who is mafia and who is not.

Second network will estimate role for each player based on all previous actions, knowing only information he is allowed to know who is mafia and who is a red player (for example mafia knows who is mafia and red players do not, for red player he knows he is a red player and actions of other players).


I am thinking about solving this game for one "team" for example, for Mafia I will implement very simple algorithm - mafia always nominate citizen, vote for
    the citizen and then kill citizen during night. I want to create algorithm for citizens to beat that simple algorithm, then I will train "Mafia" policy against that citizen agent to improve win rate and iterate between sides until I cannot improve each side.






Your approach to modeling the game using reinforcement learning seems to be on the right track. Here are a few suggestions to improve it:

1. **Modeling Declarations**: The declarations "I think X is a red/black player" can be modeled as part of the state of the game. Each player can have a belief vector that represents their belief about the role of each other player. This belief vector can be updated based on the declarations made by other players. The action of making a declaration can then be modeled as an action that influences the belief vectors of other players.

2. **Policy Network**: The policy network can take as input the current state of the game (including the belief vectors of all players) and output a probability distribution over the possible actions. The actions can include making a declaration about another player, nominating a player for voting, or casting a vote. The policy network can be trained to maximize the expected future reward, which is the probability of winning the game.

3. **Value Network**: The value network can take as input the current state of the game and output an estimate of the expected future reward. This can be used to guide the policy network in choosing actions that are likely to lead to winning the game.

4. **Role Estimation Network**: The role estimation network can take as input the history of actions and declarations made by a player and output a probability distribution over the possible roles of that player. This can be used to update the belief vector of the player making the estimation.

5. **Exploration vs Exploitation**: In reinforcement learning, there is a trade-off between exploration (trying out new actions to see their effects) and exploitation (choosing the action that is currently believed to be the best). This can be managed using techniques like epsilon-greedy exploration, where a random action is chosen with probability epsilon, and the action suggested by the policy network is chosen with probability 1-epsilon.

6. **Training**: The networks can be trained using a reinforcement learning algorithm like Q-learning or policy gradient. The training can be done by playing many games against each other and updating the networks based on the outcomes of the games. The training can be sped up by using techniques like experience replay, where past experiences are stored and randomly sampled for training, and target networks, where a separate network is used to estimate the future reward for stability.

7. **Evaluation**: The performance of the networks can be evaluated by playing games against each other or against human players. The evaluation can be done periodically during training to track the progress and adjust the training parameters if necessary.
-----

>>>How I can provide data to the Role Estimation Network with self play? I presume when I just start training
    policy network will perform poorly and how Role Estimation Network can improve when there's no quality da
   ta to train on (I presume Role Estimation Network will find patterns in play sequences which will indicate
    that cetrain player "red" or "black"?
You're correct that initially, the policy network will perform poorly and the data generated through self-play may not be of high quality. However, as the policy network improves over time, the quality of the data will
 also improve. This is a common challenge in reinforcement learning and is often addressed through iterative training processes.

Here's a possible approach:

1. **Initialize**: Start by initializing the policy network with random weights. The Role Estimation Network can also be initialized in a similar way.

2. **Generate Data**: Use the policy network to play games against itself and generate data. This data includes the actions taken by each player and the outcomes of the games.

3. **Train Role Estimation Network**: Use the generated data to train the Role Estimation Network. Even though the data is not of high quality, it can still provide useful information for the network to learn from. For
 example, if a player consistently makes incorrect declarations, the network can learn that this player is likely to be a black player.

4. **Update Policy Network**: Use the Role Estimation Network to update the policy network. The policy network can use the role estimates to make better decisions about which actions to take.

5. **Iterate**: Repeat the process of generating data, training the Role Estimation Network, and updating the
 policy network. Over time, both networks should improve and the quality of the data should increase.

This process can be seen as a form of co-training, where the two networks are trained together and help each other improve. It's important to note that the training process may be slow and require a large number of iterations, especially in the early stages when the data is of low quality. However, with enough iterations, both networks should be able to learn effective strategies for the game.
